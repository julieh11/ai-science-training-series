{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlwRQM4dD6oGeY+JbrzZNb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julieh11/ai-science-training-series/blob/main/Homework_07_AI_Accelerators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Homework\n",
        "\n",
        "**What are the key architectural features that make these systems suitable for AI workloads?**\n",
        "\n",
        "•\tThe cores are not separated from memory, which speeds up computational time of fetching operands from the memory.\n",
        "\n",
        "•\tImplementations of dataflow architectures where task executions are dependent on the data speed up operation processes\n",
        "\n",
        "•\tThe chips tend to be larger than common GPUs but contain large amounts of transistors and memory storage.\n",
        "\n",
        "•\tA program can be represented in terms of a program graph rather than step by step. These program graphs can then be distributed across multiple chips.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Identify the primary differences between these AI accelerator systems in terms of their architecture and programming models.**\n",
        "\n",
        "•\tCerebras WSE: implements a wafer scale approach, making the chip significantly larger than a typical CPU. On the other hand, the memory is part of the chip and it contains trillions of transistors. Each core has an equal amount of memory allocated to it, as well as a processing element, and a fabric router which can communicate with neighboring processors. Colors, routes and wavelets are used to facilitate data transferring between neighboring chips. It is a data flow-based chip which allows tasks to be activated by the arrival of the data packets. SawramX and MemoryX nodes are used for storing data. CSTorch can be utilized to load data or implement functions on chips.\n",
        "\n",
        "•\tGraphcore Intelligent Processing Unit (IPU):  utilizes multiple instructions and multiple data architectures. Through the bulk-synchronous parallel (BSP) model, task executions can be split into steps. Each step is made up of sync, compute, and exchange phases. IPU links are used to connect IPUs with each other.\n",
        "\n",
        "•\tSambaNova Reconfigurable Dataflow Unit (RDU):  computer units and memory units are connected through a mesh. Samba PyTorch is used to integrate software into the architecture. It compiles the PyTorch program and creates a PEF, which is then executed on underlying RDUs.\n",
        "\n",
        "•\tGroq LPU: the software of the chip is specifically targeted towards inference. Its building blocks consists of specialized SIMD units (matrix and vector multiply units, vector-vector operations, data reshapes, and on-chip SRAM).  Trained models can be exported onto Onix to then undergo GroqFlow pipeline. A downside of this chip is that it does not allow for training to be performed on it.\n",
        "\n",
        "\n",
        "\n",
        "**Based on hands-on sessions, describe a typical workflow for refactoring an AI model to run on one of ALCF's AI testbeds (e.g., SambaNova or Cerebras). What tools or software stacks are typically used in this process?**\n",
        "\n",
        "•\tMost of these testbeds have their own spin on PyTorch. A typical workflow could be generalized to defining a model that is compatible with the specific API, making adjustments to take advantage of specific hardware architecture, followed by compilation and runtime.\n",
        "\n",
        "\n",
        "**Give an example of a project that would benefit from AI accelerators and why?**\n",
        "\n",
        "•\tThe simulation and analysis of plasma behavior to better understand plasma instabilities could benefit from AI accelerators since this requires a lot of computational resources. Data from plasma experiments is often times noisy, making detection of weak signals difficult. AI accelerators could speed up and optimize signal processing.\n"
      ],
      "metadata": {
        "id": "dYJQxQBcdp4E"
      }
    }
  ]
}